{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\armlo\\anaconda3\\Lib\\site-packages\\pyspark\\pandas\\__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.pandas import DataFrame\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\n",
    "                \"spark.jars.packages\", \n",
    "                \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.1\",\n",
    "        ) \\\n",
    "        .config(\n",
    "                \"spark.sql.extensions\",\n",
    "                \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "        ) \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "        .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "        .config(\"spark.sql.catalog.local.warehouse\", \"warehouse\") \\\n",
    "        .getOrCreate()\n",
    "        # .config(\"spark.sql.warehouse.dir\", \"warehouse\") \\\n",
    "        # .config(\"spark.sql.defaultCatalog\", \"local\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ SOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read \\\n",
    "#     .options(\n",
    "#         header=True,\n",
    "#         inferSchema=True,\n",
    "#         sep=\",\"\n",
    "#     ) \\\n",
    "#     .csv(\"src/animes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let create some dataframe\n",
    "from pyspark.sql.types import StructType, StructField, LongType, FloatType, DoubleType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"vendor_id\", LongType(), True),\n",
    "    StructField(\"trip_id\", LongType(), True),\n",
    "    StructField(\"trip_distance\", FloatType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True)\n",
    "])\n",
    "data = [\n",
    "    (1, 1000371, 1.8, 15.32, \"N\"),\n",
    "    (2, 1000372, 2.5, 22.15, \"N\"),\n",
    "    (2, 1000373, 0.9, 9.01, \"N\"),\n",
    "    (1, 1000374, 8.4, 42.13, \"Y\")\n",
    "]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Lake House Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.writeTo(\"example_test\").create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Lake House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema\n",
    "spark.table(\"example_test\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the new Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = spark.table(\"example_test\").schema\n",
    "new_data = [\n",
    "    (9, 1000999, 9.9, 99.99, \"Y\")\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.union(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeTo(\"example_test\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"example_test\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+-----------+------------------+\n",
      "|vendor_id|trip_id|trip_distance|fare_amount|store_and_fwd_flag|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "|        2|1000373|          0.9|       9.01|                 N|\n",
      "|        1|1000371|          1.8|      15.32|                 N|\n",
      "|        2|1000372|          2.5|      22.15|                 N|\n",
      "|        1|1000374|          8.4|      42.13|                 Y|\n",
      "|        9|1000999|          9.9|      99.99|                 Y|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df[\"vendor_id\"] != 1).select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+-----------+------------------+\n",
      "|vendor_id|trip_id|trip_distance|fare_amount|store_and_fwd_flag|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "|        2|1000372|          2.5|      22.15|                 N|\n",
      "|        2|1000373|          0.9|       9.01|                 N|\n",
      "|        9|1000999|          9.9|      99.99|                 Y|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeTo(\"local.example_test\").replace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+-----------+------------------+\n",
      "|vendor_id|trip_id|trip_distance|fare_amount|store_and_fwd_flag|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "|        2|1000373|          0.9|       9.01|                 N|\n",
      "|        2|1000372|          2.5|      22.15|                 N|\n",
      "|        9|1000999|          9.9|      99.99|                 Y|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM \n",
    "        local.example_test\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "INSERT INTO local.example_test\n",
    "VALUES (1, 1000371, 2, 20, 'N'), \n",
    "(2, 1000372, 4, 39, 'Y');    \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        CREATE OR REPLACE TABLE new_table_example(\n",
    "            vendor_id bigint,\n",
    "            trip_id bigint,\n",
    "            trip_distance float,\n",
    "            fare_amount double,\n",
    "            store_and_fwd_flag string\n",
    "        ) USING iceberg\n",
    "        PARTITIONED BY (vendor_id);\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    INSERT INTO new_table_example (\n",
    "        SELECT * FROM example_test\n",
    "    )\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+-----------+------------------+\n",
      "|vendor_id|trip_id|trip_distance|fare_amount|store_and_fwd_flag|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "|        9|1000999|          9.9|      99.99|                 Y|\n",
      "|        1|1000371|          2.0|       20.0|                 N|\n",
      "|        2|1000373|          0.9|       9.01|                 N|\n",
      "|        2|1000372|          2.5|      22.15|                 N|\n",
      "|        2|1000372|          4.0|       39.0|                 Y|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM new_table_example\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2024-05-05 16:30:...|7756389368573949685|               NULL|   append|warehouse/example...|{spark.app.id -> ...|\n",
      "|2024-05-05 16:39:...|8750744565593823205|7756389368573949685|   append|warehouse/example...|{spark.app.id -> ...|\n",
      "|2024-05-05 20:23:...|8121411867246861750|8750744565593823205|   append|warehouse/example...|{spark.app.id -> ...|\n",
      "| 2024-05-05 20:26:20|4456078782926854740|               NULL|   append|warehouse/example...|{spark.app.id -> ...|\n",
      "|2024-05-05 20:28:...|6201281357802780287|4456078782926854740|   append|warehouse/example...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM example_test.snapshots\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+-----------+------------------+\n",
      "|vendor_id|trip_id|trip_distance|fare_amount|store_and_fwd_flag|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "|        9|1000999|          9.9|      99.99|                 Y|\n",
      "|        1|1000371|          2.0|       20.0|                 N|\n",
      "|        2|1000373|          0.9|       9.01|                 N|\n",
      "|        2|1000372|          2.5|      22.15|                 N|\n",
      "|        2|1000372|          4.0|       39.0|                 Y|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Version 5\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT * FROM new_table_example\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[previous_snapshot_id: bigint, current_snapshot_id: bigint]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "        CALL system.rollback_to_snapshot('example_test', 4456078782926854740)\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+-----------+------------------+\n",
      "|vendor_id|trip_id|trip_distance|fare_amount|store_and_fwd_flag|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "|        2|1000373|          0.9|       9.01|                 N|\n",
      "|        2|1000372|          2.5|      22.15|                 N|\n",
      "|        9|1000999|          9.9|      99.99|                 Y|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rollback to version 4 (current version 6)\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "        SELECT *\n",
    "        FROM example_test\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
