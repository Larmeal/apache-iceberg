{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCS bucket \\\n",
    "creation\n",
    "can be using any config but make sure that GCS will having same region as Dataproc\\\n",
    "In this workshop will be using \"dev-test-services-lakehouse-training\"\\\n",
    "in region \"us-central1\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data proc \\\n",
    "creation will be using single machine in \"us-central1\"\\\n",
    "with spark version 3.3 \\\n",
    "name \"lakehouse-workshop\" \\\n",
    "Enable \"component gateway\", adding component \"Jupyter Notebook\", \"Zeppelin Notebook\n",
    "\"\\\n",
    "customerize cluster to use bucket that we made in above mention,\\\n",
    "in this workshop we use \"dev-test-services-lakehouse-training\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biglake \\\n",
    "to create Biglake connection go to bigquery \\\n",
    "Go to +ADD and choose external data source \\\n",
    "connnection type BViglake and remote function (cloud resource) \\\n",
    "create connection ID \"biglake_iceberg\" \\\n",
    "region \"us-central1\" \n",
    "#https://cloud.google.com/bigquery/docs/iceberg-tables#bq_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Start with pyspark session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"iceberg\")\n",
    "    .master(\"local\")\n",
    "    .config(\n",
    "    \"spark.jars.packages\",\n",
    "    \"\"\"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.0\"\"\",    \n",
    "    )\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\")\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"gs://arm-test-lakehouse/warehouse\")\n",
    "    .config(\"spark.sql.defaultCatalog\", \"local\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "#spark session documents\n",
    "#https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/spark_session.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#let create some dataframe\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "  StructField(\"vendor_id\", LongType(), True),\n",
    "  StructField(\"trip_id\", LongType(), True),\n",
    "  StructField(\"trip_distance\", FloatType(), True),\n",
    "  StructField(\"fare_amount\", DoubleType(), True),\n",
    "  StructField(\"store_and_fwd_flag\", StringType(), True)\n",
    "])\n",
    "data = [\n",
    "    (1, 1000371, 1.8, 15.32, \"N\"),\n",
    "    (2, 1000372, 2.5, 22.15, \"N\"),\n",
    "    (2, 1000373, 0.9, 9.01, \"N\"),\n",
    "    (1, 1000374, 8.4, 42.13, \"Y\")\n",
    "  ]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#let save it as a file \n",
    "df.writeTo(\"nyc_taxis\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#let have a look at schema in Iceberg table\n",
    "schema = spark.table(\"nyc_taxis\").schema\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#try adding new row to iceberg table\n",
    "schema = spark.table(\"nyc_taxis\").schema\n",
    "data = [\n",
    "    (9, 1000999, 9.9, 99.99, \"Y\")\n",
    "  ]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.writeTo(\"nyc_taxis\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#have look at new record\n",
    "df = spark.table(\"nyc_taxis\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#connect Biglake to Iceberg table in GCS \\\n",
    "First have a look at GCS that we connect with dataproc you will see warehouse directory with each table inside \\\n",
    "also each table will contain data and metadata \\\n",
    "to connect with Iceberg table use below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "-- Bigquery\n",
    "\n",
    "  CREATE EXTERNAL TABLE dev-test-services.lake__workshop.lake_workshop_iceberg\n",
    "  WITH CONNECTION `dev-test-services.us-central1.biglake_iceberg`\n",
    "  OPTIONS (\n",
    "         format = 'ICEBERG',\n",
    "         uris = [\"gs://dev-test-services-lake-workshop-m/warehouse/taxis_pyspark_ice/metadata/v2.metadata.json\"]\n",
    "   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#try create table using spark sql\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "CREATE OR REPLACE TABLE demo.nyc.taxis\n",
    "(\n",
    "  vendor_id bigint,\n",
    "  trip_id bigint,\n",
    "  trip_distance float,\n",
    "  fare_amount double,\n",
    "  store_and_fwd_flag string\n",
    ")  USING iceberg\n",
    "PARTITIONED BY (vendor_id);\n",
    " \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "INSERT INTO demo.nyc.taxis\n",
    "VALUES (1, 1000371, 1.8, 15.32, 'N'), (2, 1000372, 2.5, 22.15, 'N'), (2, 1000373, 0.9, 9.01, 'N'), (1, 1000374, 8.4, 42.13, 'Y');\n",
    " \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#query data\n",
    "spark.sql(\"\"\"\n",
    "    select * from nyc_taxis\n",
    "\"\"\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#create new table\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "CREATE OR REPLACE TABLE new_data\n",
    "(\n",
    "  vendor_id bigint,\n",
    "  trip_id bigint,\n",
    "  trip_distance float,\n",
    "  fare_amount double,\n",
    "  store_and_fwd_flag string\n",
    ")  USING iceberg\n",
    "PARTITIONED BY (vendor_id); \n",
    " \n",
    "\"\"\"\n",
    ")\n",
    " \n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "INSERT INTO new_data\n",
    "(\n",
    "VALUES (1, 1000371, 2, 20, 'N'), (2, 1000372, 4, 39, 'Y'));\n",
    " \n",
    "\"\"\"\n",
    ")\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT * FROM new_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#try merge new and old data\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO nyc_taxis target\n",
    "    USING (SELECT * FROM new_data) source\n",
    "    ON target.trip_id = source.trip_id\n",
    "    WHEN MATCHED\n",
    "        THEN UPDATE SET target.trip_distance = source.trip_distance, target.fare_amount = source.fare_amount, target.store_and_fwd_flag = source.store_and_fwd_flag\n",
    "    WHEN NOT MATCHED\n",
    "        THEN INSERT *;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#try on schema Evolution\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "CREATE OR REPLACE TABLE taxis\n",
    "(\n",
    "  vendor_id bigint,\n",
    "  trip_id bigint,\n",
    "  trip_distance float,\n",
    "  fare_amount double,\n",
    "  store_and_fwd_flag string\n",
    ")  USING iceberg\n",
    "PARTITIONED BY (vendor_id);\n",
    " \n",
    "\"\"\"\n",
    ")\n",
    " \n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "INSERT INTO taxis\n",
    "VALUES (1, 1000371, 1.8, 15.32, 'N'), (2, 1000372, 2.5, 22.15, 'N'), (2, 1000373, 0.9, 9.01, 'N'), (1, 1000374, 8.4, 42.13, 'Y');\n",
    " \n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE taxis RENAME COLUMN fare_amount TO fare\n",
    "\"\"\")\n",
    "df = spark.table(\"demo.nyc.taxis\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#change trip distance to distance\n",
    "spark.sql(\"\"\" \n",
    "    ALTER TABLE taxis RENAME COLUMN trip_distance TO distance\n",
    "\"\"\")\n",
    "\n",
    "df = spark.table(\"taxis\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#add comment COLUMN\n",
    "spark.sql(\"\"\" \n",
    "   ALTER TABLE taxis ALTER COLUMN distance COMMENT 'The elapsed trip distance in miles reported by the taximeter.'\n",
    "\"\"\")\n",
    "spark.sql( \"\"\" \n",
    "  DESCRIBE EXTENDED taxis\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#change data type\n",
    "spark.sql(\n",
    "  \"\"\" \n",
    " ALTER TABLE taxis ALTER COLUMN distance TYPE double;\n",
    "\"\"\")\n",
    "spark.sql(\n",
    "  \"\"\" \n",
    " DESCRIBE EXTENDED taxis\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#change order\n",
    "spark.sql(\n",
    "  \"\"\" \n",
    " ALTER TABLE taxis ALTER COLUMN distance AFTER fare;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#add new COLUMN\n",
    "spark.sql(\n",
    "  \"\"\" \n",
    " ALTER TABLE taxis\n",
    "ADD COLUMN fare_per_distance_unit float AFTER distance\n",
    "\"\"\")\n",
    "df = spark.table(\"taxis\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#add data to new COLUMN\n",
    "spark.sql(\n",
    "  \"\"\" \n",
    "UPDATE taxis\n",
    "SET fare_per_distance_unit = fare/distance\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#delete COLUMN\n",
    "spark.sql(\"\"\"\n",
    "    select * from taxis\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE taxis DROP COLUMN store_and_fwd_flag\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Partition evolution change partition without need to create new TABLE\n",
    "spark.sql(\n",
    "  \"\"\" \n",
    "ALTER TABLE nyc_taxis\n",
    "ADD PARTITION FIELD trip_id\n",
    "\"\"\")\n",
    "spark.sql(\n",
    "  \"\"\" \n",
    "DESCRIBE EXTENDED nyc_taxis\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#query snapshot id to rollback table\n",
    "spark.sql(\n",
    "  \"\"\" \n",
    "\tSELECT snapshot_id, manifest_list\n",
    "\tFROM nyc_taxis.snapshots\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#check table history\n",
    "spark.sql(\n",
    "  \"\"\" \n",
    "\tSELECT *\n",
    "\tFROM nyc_taxis\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#rollback table must fill in snapshot id\n",
    "spark.sql(f\"\"\"\n",
    "  CALL system.rollback_to_snapshot('nyc_taxis', {snapshotid})\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "  \"\"\" \n",
    "SELECT *\n",
    "FROM nyc_taxis\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
